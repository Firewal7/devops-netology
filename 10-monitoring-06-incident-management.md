### Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Задание

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информацию о сбое можно изучить по ссылкам ниже:

- краткое описание на русском языке;
- развёрнутое описание на английском языке.


## Краткое описание инцидента

    21 октября 2018 года, в 22:52 UTC множество сервисов GitHub пострадали от сбоев сети и проблем в работе БД. В результате множество пользователей наблюдали устаревшую или неполную информацию на сайте GitHub.com. В период восстановления хостинга в работу, пришлось остановить работу веб-хуков, что так же могло доставлять пользователям неудобства.

## Предшествующие события

    Техобслуживание сети, для замены оптического оборудования.

## Причина инцидента

    Основная причина - неверная в рамках инфраструктуры настройка репликатора MySQL orchestrator. Тригером инцидента послужила потеря связности между ядром сети (network hub) и основным датацентром на восточном побережье США, Связность была восстановлена через 43 секунды, но это запустило цепочку событий, в результате которых развалились кластеры БД.

## Воздействие

    В течение 24 часов и 11 минут 100% пользователей испытывали проблемы при работе с сайтом. Не работали Issues, Webhooks, невозможно было создать и разместить странички на GitHub Pages, на сайте отображалась устаревшая или неполная информация.

## Обнаружение

    В 22:54 система мониторинга начала отправлять алерты об многочисленных ошибках в системе. В 23:02 дежурные инженеры определили, что множество кластеров баз данных находились в неожиданном (unexpected) состоянии.

## Реакция

    Дежурные инженеры вручную остановили деплой и перевели систему в статус "желтый".

    Был подключен координатор по инцидентам, который изменил статус сервиса на "красный".

    Подключили дополнительных инженеров из команды администрирования БД.

    Отключили часть сервисов для пользователей, чтобы снизить нагрузку на кластер и обезопасить систему от дальнейшей потери пользовательских данных.

## Восстановление

    Инженеры разработали план по восстановлению основного кластера на восточном побережье из резервной копии и последующей репликации актуальных данных с кластера на западном побережье.

    В середине следующего рабочего дня, когда нагрузка на сервис стала пиковой, развернули дополнительные кластеры БД.

    На последнем этапе были возвращены в работу отключенные сервисы, и скопившиеся в бэклоге задачи по вызову веб-хуков и публикации GitHub Pages начали выполняться, это заняло более 6 часов.

## Таймлайн

    2018 October 21 22:52 UTC: во время падения сети, развалился кластер БД, после восстановления связи в двух частях кластера оказались уникальные данные;

    2018 October 21 22:54 UTC: система мониторинга начала оповещать инженеров о многочисленных ошибках;

    2018 October 21 23:02 UTC: дежурные инженеры обнаружили что множество кластеров БД оказались в "неожиданном" состоянии;

    2018 October 21 23:07 UTC: команда залочила утилиты деплоя, для предотвращения дополнительных возможных изменений;

    2018 October 21 23:19 UTC: команда присвоила "желтый" статус системе для эскалации проблемы, координатору по инцидентам был отправлен алерт;

    2018 October 21 23:11 UTC: координатор подключился к команде, спустя две минуты изменил статус на "красный";

    2018 October 21 23:13 UTC: подключили дополнительных инженеров из команды обслуживания БД;

    2018 October 21 23:19 UTC: инженеры намерено остановили работу веб-хуков и сборку GitHub Pages чтобы не подвергать данные
    пользователей дальнейшей опасности;

    2018 October 22 00:05 UTC: инженеры начали разработку плана по возврату кластера БД к консистентному состоянию, основная сложность была в объёме данных;

    2018 October 22 00:41 UTC: начался процесс восстановления из бекапа, параллельно инженеры искали способы ускорить передачу данных;

    2018 October 22 06:51 UTC: несколько кластеров завершили восстановление из бекапов в датацентре на восточном побережье и начали репликацию данных с датацентра на западном побережье;

    2018 October 22 07:46 UTC: GutHub опубликовали пост в блоге, чтобы донести пользователям больше информации. Это потребовало особых усилий, так блог использует GutHub Pages, сборка страниц для которого ранее была остановлена;

    2018 October 22 11:12 UTC: восстановился основной кластер БД на восточном побережье, что сделало систему гораздо более отзывчивой, т.к. сами приложения развёрнуты в том же датацентре. При этом множество read-only баз продолжали репликацию, отставая от мастера на несколько часов, по этой причине множество пользователей получали неконсистентные данные. Стало понятно, что восстановление займёт больше времени, так как пользователи пришли в понедельник на работу и начали работу с сервисом, создавая новые данные;

    2018 October 22 13:15 UTC: зафиксирован пик нагрузки трафика на GitHub.com, разрыв между мастером БД и репликами стал увеличиваться, вместо того чтобы сокращаться. Инженеры приняли решения развернуть дополнительные реплики MySQL на чтение;

    2018 October 22 16:24 UTC: синхронизация реплик завершилась, инициирован возврат к исходной топологии;

    2018 October 22 16:45 UTC: началась обработка накопившегося стека событий, в беклоге были отложены миллион ивентов в хуках и 80 тысяч запросов на билд GitHub Pages;

    2018 October 22 23:03 UTC: работа веб-хуков и GutHub Pages восстановлена, связность всех систем подтверждена;

## Последующие действия

    В процессе восстановления, были собраны несколько журналов MySQL с данными, которые не были реплицированы на западное побережье. Проводится анализ чтобы определить, сколько их них получится восстановить автоматический.
    Поправить конфигурацию Orchestrator чтобы предотвратит передачу primary роли между регионами.
    Переработать систему оповещения о статусе системы, сделать её более информативной, оповещать о статусе отдельных компонентах сервиса.
    За несколько недель до инцидента, началась работа по улучшению отказоустойчивости сервиса, чтобы система оставалась доступной даже при отказе целого датацентра.
    Компания займёт более проактивную позицию в тестировании собственных предположений (примечание по переводу: вероятно, в контексте отказоустойчивости и т.д.)
    Компания будет инвестировать больше в chaos engineering, тестируя различные сценарии отказа.


---

### Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.